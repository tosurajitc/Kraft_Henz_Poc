# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: public/llmcloud/requestmanager/v1/requestmanager.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder

# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from buf.validate import validate_pb2 as buf_dot_validate_dot_validate__pb2
from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2

DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(
    b'\n6public/llmcloud/requestmanager/v1/requestmanager.proto\x12!public.llmcloud.requestmanager.v1\x1a\x1b\x62uf/validate/validate.proto\x1a\x1cgoogle/api/annotations.proto"\xd9\x01\n\x1fGetTextCompletionStreamResponse\x12"\n\nrequest_id\x18\x01 \x01(\tH\x00R\trequestId\x88\x01\x01\x12\x1d\n\x07\x63ontent\x18\x02 \x01(\tH\x01R\x07\x63ontent\x88\x01\x01\x12N\n\x05stats\x18\x03 \x01(\x0b\x32\x33.public.llmcloud.requestmanager.v1.PerformanceStatsH\x02R\x05stats\x88\x01\x01\x42\r\n\x0b_request_idB\n\n\x08_contentB\x08\n\x06_stats"\xd6\t\n\x18GetTextCompletionRequest\x12!\n\x08model_id\x18\x01 \x01(\tB\x06\xbaH\x03\xc8\x01\x01R\x07modelId\x12(\n\rsystem_prompt\x18\x02 \x01(\tH\x00R\x0csystemPrompt\x88\x01\x01\x12\x64\n\x07history\x18\x03 \x03(\x0b\x32J.public.llmcloud.requestmanager.v1.GetTextCompletionRequest.HistoryMessageR\x07history\x12\'\n\x0buser_prompt\x18\x04 \x01(\tB\x06\xbaH\x03\xc8\x01\x01R\nuserPrompt\x12\x17\n\x04seed\x18\x05 \x01(\rH\x01R\x04seed\x88\x01\x01\x12-\n\nmax_tokens\x18\x06 \x01(\rB\t\xbaH\x06*\x04\x10\x80\x80\x04H\x02R\tmaxTokens\x88\x01\x01\x12\x36\n\x0btemperature\x18\x07 \x01(\x02\x42\x0f\xbaH\x0c\n\n\x1d\x00\x00\x00@%\x00\x00\x00\x00H\x03R\x0btemperature\x88\x01\x01\x12)\n\x05top_p\x18\x08 \x01(\x02\x42\x0f\xbaH\x0c\n\n\x1d\x00\x00\x80?-\x00\x00\x00\x00H\x04R\x04topP\x88\x01\x01\x12%\n\x05top_k\x18\t \x01(\rB\x0b\xbaH\x08*\x06\x18\x80\xfa\x01(\x00H\x05R\x04topK\x88\x01\x01\x12\x30\n\x11\x66requency_penalty\x18\n \x01(\x02H\x06R\x10\x66requencyPenalty\x88\x01\x01\x12i\n\nlogit_bias\x18\x0b \x03(\x0b\x32J.public.llmcloud.requestmanager.v1.GetTextCompletionRequest.LogitBiasEntryR\tlogitBias\x12\x11\n\x01n\x18\x0c \x01(\rH\x07R\x01n\x88\x01\x01\x12.\n\x10presence_penalty\x18\r \x01(\x02H\x08R\x0fpresencePenalty\x88\x01\x01\x12\x12\n\x04stop\x18\x0e \x03(\tR\x04stop\x12-\n\x10max_input_tokens\x18\x0f \x01(\rH\tR\x0emaxInputTokens\x88\x01\x01\x1a`\n\x0eHistoryMessage\x12\x1f\n\x0buser_prompt\x18\x01 \x01(\tR\nuserPrompt\x12-\n\x12\x61ssistant_response\x18\x02 \x01(\tR\x11\x61ssistantResponse\x1a<\n\x0eLogitBiasEntry\x12\x10\n\x03key\x18\x01 \x01(\x05R\x03key\x12\x14\n\x05value\x18\x02 \x01(\x05R\x05value:\x02\x38\x01:\xb4\x01\xbaH\xb0\x01\x1a\xad\x01\n(get_text_completion_request.query_length\x12\x45user_prompt length plus system_prompt length must be less than 262144\x1a:size(this.user_prompt) + size(this.system_prompt) < 262144B\x10\n\x0e_system_promptB\x07\n\x05_seedB\r\n\x0b_max_tokensB\x0e\n\x0c_temperatureB\x08\n\x06_top_pB\x08\n\x06_top_kB\x14\n\x12_frequency_penaltyB\x04\n\x02_nB\x13\n\x11_presence_penaltyB\x13\n\x11_max_input_tokens"\x9f\x01\n\x19GetTextCompletionResponse\x12\x1d\n\nrequest_id\x18\x01 \x01(\tR\trequestId\x12\x18\n\x07\x63ontent\x18\x02 \x01(\tR\x07\x63ontent\x12I\n\x05stats\x18\x03 \x01(\x0b\x32\x33.public.llmcloud.requestmanager.v1.PerformanceStatsR\x05stats"\xb6\x01\n\x10PerformanceStats\x12%\n\x0etime_generated\x18\x01 \x01(\x01R\rtimeGenerated\x12)\n\x10tokens_generated\x18\x02 \x01(\rR\x0ftokensGenerated\x12%\n\x0etime_processed\x18\x03 \x01(\x01R\rtimeProcessed\x12)\n\x10tokens_processed\x18\x04 \x01(\rR\x0ftokensProcessed2\xac\x03\n\x15RequestManagerService\x12\xcc\x01\n\x17GetTextCompletionStream\x12;.public.llmcloud.requestmanager.v1.GetTextCompletionRequest\x1a\x42.public.llmcloud.requestmanager.v1.GetTextCompletionStreamResponse".\x82\xd3\xe4\x93\x02("#/v1/request_manager/text_completion:\x01*0\x01\x12\xc3\x01\n\x11GetTextCompletion\x12;.public.llmcloud.requestmanager.v1.GetTextCompletionRequest\x1a<.public.llmcloud.requestmanager.v1.GetTextCompletionResponse"3\x82\xd3\xe4\x93\x02-"(/v1/request_manager/text_completion_full:\x01*B\xb4\x02\n%com.public.llmcloud.requestmanager.v1B\x13RequestmanagerProtoP\x01ZMgit.groq.io/cloud/go-proto/public/llmcloud/requestmanager/v1;requestmanagerv1\xa2\x02\x03PLR\xaa\x02!Public.Llmcloud.Requestmanager.V1\xca\x02"Public_\\Llmcloud\\Requestmanager\\V1\xe2\x02.Public_\\Llmcloud\\Requestmanager\\V1\\GPBMetadata\xea\x02$Public::Llmcloud::Requestmanager::V1b\x06proto3'
)

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(
    DESCRIPTOR, "public.llmcloud.requestmanager.v1.requestmanager_pb2", _globals
)
if _descriptor._USE_C_DESCRIPTORS == False:
    _globals["DESCRIPTOR"]._options = None
    _globals[
        "DESCRIPTOR"
    ]._serialized_options = b'\n%com.public.llmcloud.requestmanager.v1B\023RequestmanagerProtoP\001ZMgit.groq.io/cloud/go-proto/public/llmcloud/requestmanager/v1;requestmanagerv1\242\002\003PLR\252\002!Public.Llmcloud.Requestmanager.V1\312\002"Public_\\Llmcloud\\Requestmanager\\V1\342\002.Public_\\Llmcloud\\Requestmanager\\V1\\GPBMetadata\352\002$Public::Llmcloud::Requestmanager::V1'
    _globals["_GETTEXTCOMPLETIONREQUEST_LOGITBIASENTRY"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST_LOGITBIASENTRY"]._serialized_options = b"8\001"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["model_id"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "model_id"
    ]._serialized_options = b"\272H\003\310\001\001"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["user_prompt"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "user_prompt"
    ]._serialized_options = b"\272H\003\310\001\001"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["max_tokens"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "max_tokens"
    ]._serialized_options = b"\272H\006*\004\020\200\200\004"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["temperature"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "temperature"
    ]._serialized_options = b"\272H\014\n\n\035\000\000\000@%\000\000\000\000"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["top_p"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "top_p"
    ]._serialized_options = b"\272H\014\n\n\035\000\000\200?-\000\000\000\000"
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name["top_k"]._options = None
    _globals["_GETTEXTCOMPLETIONREQUEST"].fields_by_name[
        "top_k"
    ]._serialized_options = b"\272H\010*\006\030\200\372\001(\000"
    _globals["_GETTEXTCOMPLETIONREQUEST"]._options = None
    _globals[
        "_GETTEXTCOMPLETIONREQUEST"
    ]._serialized_options = b"\272H\260\001\032\255\001\n(get_text_completion_request.query_length\022Euser_prompt length plus system_prompt length must be less than 262144\032:size(this.user_prompt) + size(this.system_prompt) < 262144"
    _globals["_REQUESTMANAGERSERVICE"].methods_by_name[
        "GetTextCompletionStream"
    ]._options = None
    _globals["_REQUESTMANAGERSERVICE"].methods_by_name[
        "GetTextCompletionStream"
    ]._serialized_options = (
        b'\202\323\344\223\002("#/v1/request_manager/text_completion:\001*'
    )
    _globals["_REQUESTMANAGERSERVICE"].methods_by_name[
        "GetTextCompletion"
    ]._options = None
    _globals["_REQUESTMANAGERSERVICE"].methods_by_name[
        "GetTextCompletion"
    ]._serialized_options = (
        b'\202\323\344\223\002-"(/v1/request_manager/text_completion_full:\001*'
    )
    _globals["_GETTEXTCOMPLETIONSTREAMRESPONSE"]._serialized_start = 153
    _globals["_GETTEXTCOMPLETIONSTREAMRESPONSE"]._serialized_end = 370
    _globals["_GETTEXTCOMPLETIONREQUEST"]._serialized_start = 373
    _globals["_GETTEXTCOMPLETIONREQUEST"]._serialized_end = 1611
    _globals["_GETTEXTCOMPLETIONREQUEST_HISTORYMESSAGE"]._serialized_start = 1122
    _globals["_GETTEXTCOMPLETIONREQUEST_HISTORYMESSAGE"]._serialized_end = 1218
    _globals["_GETTEXTCOMPLETIONREQUEST_LOGITBIASENTRY"]._serialized_start = 1220
    _globals["_GETTEXTCOMPLETIONREQUEST_LOGITBIASENTRY"]._serialized_end = 1280
    _globals["_GETTEXTCOMPLETIONRESPONSE"]._serialized_start = 1614
    _globals["_GETTEXTCOMPLETIONRESPONSE"]._serialized_end = 1773
    _globals["_PERFORMANCESTATS"]._serialized_start = 1776
    _globals["_PERFORMANCESTATS"]._serialized_end = 1958
    _globals["_REQUESTMANAGERSERVICE"]._serialized_start = 1961
    _globals["_REQUESTMANAGERSERVICE"]._serialized_end = 2389
# @@protoc_insertion_point(module_scope)
